import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import json
import redis.asyncio as redis
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_, olumn, Integer, String, DateTime
from sqlalchemy.orm import selectinload
import numpy as np
from collections import defaultdict

# ‚úÖ IMPORTS CORREGIDOS - Usar integration.py en lugar de leads.py
from ...models.integration import Lead, LeadStatus, ExternalLead, Integration
from ...models.interaction import Interaction, ConversationSummary
from ...models.workflow import WorkflowExecution, EmailSend

@dataclass
class MetricConfig:
    """Configuraci√≥n para m√©tricas del sistema"""
    name: str
    calculation_type: str
    refresh_interval: int
    cache_ttl: int
    aggregation_window: str

class AnalyticsEngine:
    """Motor principal de analytics con procesamiento en tiempo real"""
    
    def __init__(self, redis_client: redis.Redis, db_session: AsyncSession):
        self.redis = redis_client
        self.db = db_session
        self.metrics_config = self._initialize_metrics()
        self.pipeline_queue = asyncio.Queue()
        self.processing_tasks = []
        
    def _initialize_metrics(self) -> Dict[str, MetricConfig]:
        """Inicializa la configuraci√≥n de m√©tricas"""
        return {
            'active_leads': MetricConfig(
                name='Leads Activos',
                calculation_type='count',
                refresh_interval=300,
                cache_ttl=600,
                aggregation_window='day'
            ),
            'conversion_rate': MetricConfig(
                name='Tasa de Conversi√≥n',
                calculation_type='average',
                refresh_interval=1800,
                cache_ttl=3600,
                aggregation_window='week'
            ),
            'response_time': MetricConfig(
                name='Tiempo de Respuesta Promedio',
                calculation_type='average',
                refresh_interval=900,
                cache_ttl=1800,
                aggregation_window='day'
            ),
            'lead_velocity': MetricConfig(
                name='Velocidad de Leads',
                calculation_type='custom',
                refresh_interval=3600,
                cache_ttl=7200,
                aggregation_window='month'
            ),
            'workflow_efficiency': MetricConfig(
                name='Eficiencia de Workflows',
                calculation_type='custom',
                refresh_interval=3600,
                cache_ttl=7200,
                aggregation_window='week'
            )
        }
    
    async def start_engine(self):
        """Inicia el motor de analytics"""
        print("üöÄ Starting Analytics Engine...")
        
        # Iniciar workers de procesamiento
        for i in range(3):  # 3 workers concurrentes
            task = asyncio.create_task(self._process_pipeline())
            self.processing_tasks.append(task)
        
        # Iniciar actualizaciones autom√°ticas de m√©tricas
        asyncio.create_task(self._auto_refresh_metrics())
        
        print("‚úÖ Analytics Engine started successfully")
    
    async def stop_engine(self):
        """Detiene el motor de analytics"""
        for task in self.processing_tasks:
            task.cancel()
        await asyncio.gather(*self.processing_tasks, return_exceptions=True)
    
    async def _process_pipeline(self):
        """Worker que procesa datos del pipeline"""
        while True:
            try:
                data_batch = await self.pipeline_queue.get()
                await self._process_batch(data_batch)
                self.pipeline_queue.task_done()
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"‚ùå Error in pipeline processing: {e}")
    
    async def _process_batch(self, batch: List[Dict]):
        """Procesa un batch de datos"""
        # Agrupar por tipo de evento
        events_by_type = defaultdict(list)
        for event in batch:
            events_by_type[event.get('type', 'unknown')].append(event)
        
        # Procesar cada tipo de evento
        for event_type, events in events_by_type.items():
            await self._update_real_time_metrics(event_type, events)
    
    async def _update_real_time_metrics(self, event_type: str, events: List[Dict]):
        """Actualiza m√©tricas en tiempo real"""
        pipe = self.redis.pipeline()
        
        for event in events:
            # Incrementar contadores generales
            pipe.hincrby(f"metrics:counters:{event_type}", "total", 1)
            pipe.hincrby(f"metrics:counters:{event_type}:daily:{datetime.utcnow().date()}", "count", 1)
            
            # Guardar evento para procesamiento posterior
            event_timestamp = datetime.utcnow().timestamp()
            pipe.zadd(
                f"metrics:events:{event_type}",
                {json.dumps(event): event_timestamp}
            )
            
            # Mantener solo los √∫ltimos 1000 eventos
            pipe.zremrangebyrank(f"metrics:events:{event_type}", 0, -1000)
        
        await pipe.execute()
    
    async def ingest_data(self, data: Dict[str, Any]):
        """Ingesta de datos al pipeline de analytics"""
        # Validar y enriquecer datos
        enriched_data = await self._enrich_data(data)
        
        # A√±adir al pipeline de procesamiento
        await self.pipeline_queue.put([enriched_data])
        
        # Trigger para m√©tricas cr√≠ticas en tiempo real
        if data.get('type') in ['lead_conversion', 'lead_qualified', 'interaction_completed']:
            await self._trigger_critical_metric_update(data)
    
    async def _enrich_data(self, data: Dict) -> Dict:
        """Enriquece los datos con informaci√≥n adicional"""
        enriched = data.copy()
        enriched['timestamp'] = datetime.utcnow().isoformat()
        enriched['processing_id'] = f"{data.get('type', 'unknown')}_{datetime.utcnow().timestamp()}"
        
        # A√±adir metadatos seg√∫n el tipo
        if data.get('type') == 'lead_created':
            enriched['lead_score'] = await self._calculate_lead_score(data)
        elif data.get('type') == 'lead_conversion':
            enriched['conversion_value'] = await self._calculate_conversion_value(data)
        
        return enriched
    
    async def _calculate_lead_score(self, lead_data: Dict) -> float:
        """Calcula el score de un lead basado en datos reales"""
        try:
            # Si tenemos un lead_id, obtener datos reales de la base de datos
            if 'lead_id' in lead_data:
                stmt = select(Lead).where(Lead.id == lead_data['lead_id'])
                result = await self.db.execute(stmt)
                lead = result.scalar_one_or_none()
                
                if lead:
                    return lead.score or 0.0
            
            # Fallback a c√°lculo b√°sico
            score = 0.0
            if lead_data.get('email'):
                score += 10
            if lead_data.get('company'):
                score += 15
            if lead_data.get('job_title') in ['director', 'manager', 'ceo']:
                score += 20
            
            return min(score, 100)
            
        except Exception as e:
            print(f"Error calculating lead score: {e}")
            return 0.0
    
    async def _calculate_conversion_value(self, conversion_data: Dict) -> float:
        """Calcula el valor de una conversi√≥n"""
        base_value = conversion_data.get('amount', 0)
        
        # Aplicar multiplicadores seg√∫n el tipo de lead
        if conversion_data.get('lead_type') == 'enterprise':
            base_value *= 1.5
        elif conversion_data.get('lead_type') == 'premium':
            base_value *= 1.2
        
        return base_value
    
    async def _trigger_critical_metric_update(self, data: Dict):
        """Actualiza m√©tricas cr√≠ticas inmediatamente"""
        metric_type = data.get('type')
        
        if metric_type == 'lead_conversion':
            await self.calculate_conversion_metrics()
        elif metric_type == 'lead_qualified':
            await self.calculate_lead_quality_metrics()
        elif metric_type == 'interaction_completed':
            await self.calculate_engagement_metrics()
    
    async def _auto_refresh_metrics(self):
        """Auto-actualiza m√©tricas seg√∫n su configuraci√≥n"""
        while True:
            try:
                current_time = datetime.utcnow()
                
                for metric_key, config in self.metrics_config.items():
                    # Verificar si necesita actualizaci√≥n
                    last_update_key = f"metrics:last_update:{metric_key}"
                    last_update = await self.redis.get(last_update_key)
                    
                    should_update = False
                    if not last_update:
                        should_update = True
                    else:
                        last_update_time = datetime.fromisoformat(last_update)
                        if (current_time - last_update_time).seconds >= config.refresh_interval:
                            should_update = True
                    
                    if should_update:
                        await self._update_metric(metric_key, config)
                        await self.redis.set(last_update_key, current_time.isoformat())
                
                # Esperar antes del pr√≥ximo ciclo
                await asyncio.sleep(60)
                
            except Exception as e:
                print(f"‚ùå Error in auto refresh: {e}")
                await asyncio.sleep(60)
    
    async def _update_metric(self, metric_key: str, config: MetricConfig):
        """Actualiza una m√©trica espec√≠fica"""
        print(f"üìä Updating metric: {metric_key}")
        
        try:
            if metric_key == 'active_leads':
                value = await self.calculate_active_leads()
            elif metric_key == 'conversion_rate':
                value = await self.calculate_conversion_rate()
            elif metric_key == 'response_time':
                value = await self.calculate_avg_response_time()
            elif metric_key == 'lead_velocity':
                value = await self.calculate_lead_velocity()
            elif metric_key == 'workflow_efficiency':
                value = await self.calculate_workflow_efficiency()
            else:
                value = None
            
            if value is not None:
                # Guardar en cach√©
                await self.redis.setex(
                    f"metrics:calculated:{metric_key}",
                    config.cache_ttl,
                    json.dumps({
                        'value': value,
                        'timestamp': datetime.utcnow().isoformat(),
                        'config': config.__dict__
                    })
                )
        except Exception as e:
            print(f"‚ùå Error updating metric {metric_key}: {e}")
    
    # M√âTRICAS REALES CON BASE DE DATOS
    async def calculate_active_leads(self) -> int:
        """Calcula cantidad de leads activos"""
        stmt = select(func.count(Lead.id)).where(
            and_(
                Lead.is_active == True,
                Lead.created_at >= datetime.utcnow() - timedelta(days=30)
            )
        )
        result = await self.db.execute(stmt)
        return result.scalar() or 0
    
    async def calculate_conversion_rate(self) -> float:
        """Calcula tasa de conversi√≥n de los √∫ltimos 30 d√≠as"""
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=30)
        
        # Total de leads
        total_stmt = select(func.count(Lead.id)).where(
            and_(
                Lead.created_at >= start_date,
                Lead.created_at <= end_date
            )
        )
        total_result = await self.db.execute(total_stmt)
        total_leads = total_result.scalar() or 0
        
        # Leads convertidos
        converted_stmt = select(func.count(Lead.id)).where(
            and_(
                Lead.created_at >= start_date,
                Lead.created_at <= end_date,
                Lead.status == LeadStatus.CONVERTED
            )
        )
        converted_result = await self.db.execute(converted_stmt)
        converted_leads = converted_result.scalar() or 0
        
        if total_leads > 0:
            return round((converted_leads / total_leads) * 100, 2)
        return 0.0
    
    async def calculate_avg_response_time(self) -> float:
        """Calcula tiempo promedio de respuesta en minutos"""
        stmt = select(func.avg(Interaction.response_time_ms)).where(
            and_(
                Interaction.created_at >= datetime.utcnow() - timedelta(days=7),
                Interaction.response_time_ms.isnot(None)
            )
        )
        result = await self.db.execute(stmt)
        avg_ms = result.scalar() or 0
        
        return round(avg_ms / 60000, 2)  # Convertir a minutos
    
    async def calculate_lead_velocity(self) -> float:
        """Calcula la velocidad de generaci√≥n de leads"""
        current_month = datetime.utcnow().replace(day=1)
        next_month = (current_month + timedelta(days=32)).replace(day=1)
        last_month = (current_month - timedelta(days=1)).replace(day=1)
        
        # Leads del mes actual
        current_stmt = select(func.count(Lead.id)).where(
            and_(
                Lead.created_at >= current_month,
                Lead.created_at < next_month
            )
        )
        current_result = await self.db.execute(current_stmt)
        current_leads = current_result.scalar() or 0
        
        # Leads del mes anterior
        last_stmt = select(func.count(Lead.id)).where(
            and_(
                Lead.created_at >= last_month,
                Lead.created_at < current_month
            )
        )
        last_result = await self.db.execute(last_stmt)
        last_leads = last_result.scalar() or 0
        
        if last_leads > 0:
            velocity = ((current_leads - last_leads) / last_leads) * 100
            return round(velocity, 2)
        return 100.0 if current_leads > 0 else 0.0
    
    async def calculate_workflow_efficiency(self) -> float:
        """Calcula eficiencia de workflows (completados vs totales)"""
        stmt = select(
            func.count(WorkflowExecution.id),
            func.sum(func.cast(WorkflowExecution.status == 'completed', Integer))
        ).where(
            WorkflowExecution.started_at >= datetime.utcnow() - timedelta(days=30)
        )
        
        result = await self.db.execute(stmt)
        total, completed = result.fetchone() or (0, 0)
        
        if total > 0:
            efficiency = (completed / total) * 100
            return round(efficiency, 2)
        return 0.0
    
    # M√âTRICAS COMPUESTAS PARA DASHBOARD
    async def get_executive_dashboard(self, days: int = 30) -> Dict[str, Any]:
        """Obtiene datos completos para dashboard ejecutivo"""
        cache_key = f"dashboard:executive:{days}"
        cached = await self.redis.get(cache_key)
        
        if cached:
            return json.loads(cached)
        
        dashboard_data = {
            'summary_metrics': {
                'active_leads': await self.calculate_active_leads(),
                'conversion_rate': await self.calculate_conversion_rate(),
                'avg_response_time': await self.calculate_avg_response_time(),
                'lead_velocity': await self.calculate_lead_velocity(),
                'workflow_efficiency': await self.calculate_workflow_efficiency()
            },
            'channel_performance': await self.get_channel_performance(days),
            'conversion_funnel': await self.get_conversion_funnel(days),
            'recent_activity': await self.get_recent_activity(10)
        }
        
        # Cachear por 5 minutos
        await self.redis.setex(cache_key, 300, json.dumps(dashboard_data))
        return dashboard_data
    
    async def get_channel_performance(self, days: int) -> List[Dict[str, Any]]:
        """Obtiene performance por canal"""
        start_date = datetime.utcnow() - timedelta(days=days)
        
        stmt = select(
            Lead.source,
            func.count(Lead.id).label('total_leads'),
            func.avg(Lead.score).label('avg_score'),
            func.sum(func.cast(Lead.is_qualified, Integer)).label('qualified_leads')
        ).where(
            Lead.created_at >= start_date
        ).group_by(Lead.source)
        
        result = await self.db.execute(stmt)
        channels_data = []
        
        for source, total, avg_score, qualified in result:
            conversion_rate = (qualified / total * 100) if total > 0 else 0
            channels_data.append({
                'channel': source or 'unknown',
                'leads_count': total,
                'avg_score': float(avg_score or 0),
                'conversion_rate': round(conversion_rate, 2),
                'qualified_leads': qualified
            })
        
        return sorted(channels_data, key=lambda x: x['leads_count'], reverse=True)
    
    async def get_conversion_funnel(self, days: int) -> Dict[str, Any]:
        """Obtiene datos del funnel de conversi√≥n"""
        start_date = datetime.utcnow() - timedelta(days=days)
        
        # Contar leads por etapa
        stmt = select(
            func.count(Lead.id),
            func.sum(func.cast(Lead.is_qualified, Integer)),
            func.sum(func.cast(Lead.status == LeadStatus.HOT, Integer)),
            func.sum(func.cast(Lead.status == LeadStatus.CONVERTED, Integer))
        ).where(Lead.created_at >= start_date)
        
        result = await self.db.execute(stmt)
        total, qualified, hot, converted = result.fetchone() or (0, 0, 0, 0)
        
        return {
            'stages': [
                {'name': 'Leads Totales', 'count': total, 'percentage': 100},
                {'name': 'Cualificados', 'count': qualified, 
                 'percentage': (qualified / total * 100) if total > 0 else 0},
                {'name': 'Calientes', 'count': hot,
                 'percentage': (hot / total * 100) if total > 0 else 0},
                {'name': 'Convertidos', 'count': converted,
                 'percentage': (converted / total * 100) if total > 0 else 0}
            ],
            'conversion_rates': {
                'total_to_qualified': (qualified / total * 100) if total > 0 else 0,
                'qualified_to_hot': (hot / qualified * 100) if qualified > 0 else 0,
                'hot_to_converted': (converted / hot * 100) if hot > 0 else 0,
                'overall': (converted / total * 100) if total > 0 else 0
            }
        }
    
    async def get_recent_activity(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Obtiene actividad reciente"""
        # √öltimas interacciones
        stmt = select(Interaction).options(
            selectinload(Interaction.lead)
        ).order_by(Interaction.created_at.desc()).limit(limit)
        
        result = await self.db.execute(stmt)
        interactions = result.scalars().all()
        
        activity = []
        for interaction in interactions:
            activity.append({
                'type': 'interaction',
                'timestamp': interaction.created_at.isoformat(),
                'lead_name': interaction.lead.name if interaction.lead else 'Unknown',
                'message': interaction.user_message[:100] + '...' if interaction.user_message else 'No message',
                'platform': interaction.platform
            })
        
        return activity
    
    async def clear_cache(self, pattern: str = "metrics:*") -> int:
        """Limpia el cache de analytics"""
        keys = await self.redis.keys(pattern)
        if keys:
            return await self.redis.delete(*keys)
        return 0